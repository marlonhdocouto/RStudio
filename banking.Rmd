---
title: "Marketing Banking"
author: "Marlon Do Couto"
date: "10/30/2020"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First off, reading the necessary libraries to do EDA and modeling. Then, reading the dataset into the dataframe.. To read more about the dataset, please access the link below:
http://archive.ics.uci.edu/ml/datasets/Bank+Marketing#

```{r}
library(tidyverse)
library(reshape2)
library(DescTools)
library(randomForest)
library(partykit)
library(partykit)
library(tree)
library(libcoin)
library(rpart)
library(ROSE)
banking<-read.csv("bank-additional-full.csv",sep = ";")
head(banking,5)
str(banking)
```
I will discard the "duration" column since this is highly correlated to the outcome "y" according to the datasource. It might influence the model heavily so to have a better predictive model, I will get rid of it. Moreover, I will change the name of some of the columns to make it clear what they mean. Then I check for NA values. There is none, however, some of the columns do have UNKNOWN as answers. For now, I will leave as it.
```{r}
banking<-banking[,-11]
head(banking,5)
colnames(banking)<-c("age","job","maritalStatus","education","hasCreditDefault","hasHousingLoan","hasPersonalLoan","contactType",
                     "monthContacted","weekDayContacted","previousContacts","daysSinceContact","previousCampContact","outcomePrevious",
                     "employRate","cpi","confIndex","euribor3M","numEmployees","outcome")
summary(banking)
sum(is.na(banking))
```
Starting with exploratory analysis to check on some relationships in the data. First, I want to check the distribution of ages for those who took deposited money and those who did not. Then, I will look at the CPI at the time of call and whether there is a relationship there. Third, we look at the relationship between whether client bought product or not in this campaign and the number of contacts made to this client prior to this campaign:

```{r}

previouscontact<-banking%>%ggplot(aes(factor(outcome),previousContacts,fill=factor(outcome)))+geom_boxplot()+scale_y_continuous(trans = "log2")+
  labs(title = "Campaign outcome and Previous Contacts", x="Outcome",y="Number of Contacts")+
  theme(legend.position = "none",panel.background = element_blank(),axis.line = element_line(color = "grey"),
        axis.ticks = element_blank())

##moreover, I also want to check if there is correlation between the economic indicators and the outcome of the call:

cpi<-banking%>%ggplot(aes(factor(outcome),cpi,fill=factor(outcome)))+geom_boxplot()+labs(title = "Campaign outcome and CPI", x="Outcome",y="CPI")+
  theme(legend.position = "none",panel.background = element_blank(),axis.line = element_line(color = "grey"),
        axis.ticks = element_blank())

confindex<-banking%>%ggplot(aes(factor(outcome),confIndex,fill=factor(outcome)))+geom_boxplot()+labs(title = "Campaign outcome and Confidende Index", x="Outcome",y="Confidence")+
  theme(legend.position = "none",panel.background = element_blank(),axis.line = element_line(color = "grey"),
        axis.ticks = element_blank())

euribor<-banking%>%ggplot(aes(factor(outcome),euribor3M,fill=factor(outcome)))+geom_boxplot()+labs(title = "Campaign outcome and Euribor", x="Outcome",y="Euribor")+
  theme(legend.position = "none",panel.background = element_blank(),axis.line = element_line(color = "grey"),
        axis.ticks = element_blank())

previouscontact
cpi
confindex
euribor

#ggsave("previouscontact.png",previouscontact)
#ggsave("cpi.png",cpi)
#ggsave("confindex.png",confindex)
#ggsave("euribor.png",euribor)

```
Some insights I gather from the boxplots above:

1. People who did not contribute with a deposit received more calls in the campaign than those who deposited. This is an interesting finding: is the constant reaching out annoying customers who would otherwise have deposited?

2. Economic findings: 

  a. there are more deposits in times of less inflation which makes sense given people are less prone to spending the money they have on their     hands or other accounts; 
  b. there are more deposits in times of less confidence.This is counter-intuitive, but if you think about it, it makes sense. As people are less confident, they save more and though this is a deposit with an interest, they might feel their money is safer at the bank.
  c. The relationship between depositing and the Euribor 3-month rate needs further investigation. It does look like the majority of yeses (3rd quantile) are depositing in periods of higher rates while the majority of nos (in the 2nd quantile) are also within the same range. This might be due to the fact that the campaign was mostly done in a period in which Euribor was between 1 and 5 percent. 
  
  
Next we look at the age distributions for those who contributed and those who did not:

```{r}

banking%>%ggplot(aes(age, fill=factor(outcome)))+geom_density(alpha=0.5)+labs(fill="Outcome")+
  theme(panel.background = element_blank(),axis.line = element_line(color = "grey"),
        axis.ticks = element_blank(),axis.title = element_blank(), legend.position = c(0.75,0.8))

banking%>%ggplot(aes(daysSinceContact))+geom_histogram(binwidth = 1)

#999 here means client was not previously contacted. However, I'd like to make it more visible with remaining distribution, therefore, I will change it to -1:
banking$daysSinceContact[which(banking$daysSinceContact==999)]<-(-1)

#Now those not contacted are in the data as -1:

banking%>%ggplot(aes(daysSinceContact))+geom_histogram(binwidth = 1)+scale_y_continuous(trans = "log10")+facet_wrap(~outcome)+
  theme(panel.background = element_blank(),axis.line = element_line(color = "grey"),
  axis.ticks = element_blank(),axis.title = element_blank())
```

The distribution for age seem quite similar for both outcomes. Days since last contact do not show a clear relationship and look similar in distribution for both outcomes.

Now I want to group by maritalStatus, education, hasCreditDefault, hasHousingLoan, hasPersonalLoan for both yes and no outcomes. This might provide more insights into the demographics of customers doing the deposits with the bank.

```{r}
banking_yes<-banking%>%filter(outcome=="yes")
banking_no<-banking%>%filter(outcome=="no")
banking_yes%>%group_by(maritalStatus)%>%count()
banking_no%>%group_by(maritalStatus)%>%count()
banking_yes%>%group_by(education)%>%count()
banking_no%>%group_by(education)%>%count()
```

Now, I will do some data cleaning transforming some of the variables into factors for better analysis and also figuring out how many variable "unknown" we have in the full dataset:
```{r}
colname<-c()
unknowns<-c()
for (i in 1:ncol(banking)){
  colname<-append(colname,colnames(banking)[i])
  unknowns<-append(unknowns,(sum(banking[,i]=="unknown")))
 }
cbind(colname,unknowns)

#now I have an idea on which variables have unknowns so I will start dealing with them. I will first treat them as N/A and try imputing them with MICE:

head(banking)

for (i in 1:nrow(banking)){
  for (j in 1:ncol(banking)){
    if(banking[i,j]=="unknown"){
      banking[i,j]<-NA
    }
  }
}

head(banking)
```
Now that the values are NA, I will go ahead and impute the values with the help of MICE:
```{r}

to_factor<-c("job","maritalStatus","education","hasCreditDefault","hasHousingLoan","hasPersonalLoan","contactType","monthContacted","weekDayContacted","outcomePrevious","outcome")
for(i in to_factor){
  banking[,i]<-as.factor(banking[,i])
}

#Imputing the NA values:

library(mice)
imputed_banking<-mice(banking,where = is.na(banking))
imputed_banking
banking<-complete(imputed_banking)
head(banking)

#to confirm:
sum(is.na(banking))
write.csv(banking,"banking2.csv")
```

Now changing the levels of some of the variables since they were 1 and 2. 0 and 1 are more intuitive for No and Yes:

```{r}

levels(banking$hasCreditDefault)<-c(0,1)
levels(banking$hasHousingLoan)<-c(0,1)
levels(banking$hasPersonalLoan)<-c(0,1)
levels(banking$outcome)<-c(0,1)

chisq.test(banking$hasCreditDefault, banking$outcome)
chisq.test(banking$hasHousingLoan,banking$outcome)
chisq.test(banking$hasPersonalLoan,banking$outcome)
chisq.test(banking$monthContacted,banking$outcome)
chisq.test(banking$weekDayContacted,banking$outcome)
chisq.test(banking$outcomePrevious,banking$outcome)

```
Through the chi-squared above, it seems like a few relationships are not independent:
- having a housing loan and the outcome
- the month contacted and the outcome
- the weekday contacted and the outcome
- the outcome from a previous campaign and the outcome from this campaign.


```{r}

plot(outcome~maritalStatus, data=banking, ylab = "Outcome",col=c(11,2),main="Outcome vs Marital Status")
plot(outcome~factor(monthContacted),data=banking,ylab="Outcome",xlab="Month Contacted",col=c(11,2),main="Outcome vs Month Contacted")
plot(outcome~hasPersonalLoan,data=banking,ylab="Outcome",xlab="Personal Loan",col=c(11,2),main="Outcome vs Has Personal Loan")
plot(banking$outcome~banking$job,col=c(11,2),main="Outcome vs Occupation",par(las=2, col.lab="white"), ylab = "Outcome")
plot(outcome~outcomePrevious,data=banking,ylab="Outcome",xlab="Previous Campaign Outcome",col=c(11,2),main="Outcome vs Previous Campaign Outcome")

```

It looks like the previous campaign gives a lot of information. 

Now I will separate the train and test data and start running models. 
```{r}

banking<-banking[,-1]

set.seed(12345)
train_index<-sample(1:nrow(banking), size=0.8*nrow(banking), rep=FALSE)
test_index<-setdiff(1:nrow(banking),train_index)

#test and train for banking:
trainbanking<-banking[train_index,]
testbanking<-banking[test_index,]

```

As I analyzed the data, I came upon the difficulties of using ML with unbalanced datasets like this one. It is important to acknowledge though that the dataset is overly unbalanced. Therefore, I will apply some tools to balance the set, test models and choose the more balanced data then to improve the models accuracy later on. 

```{r}
#see unbalanced:
table(banking$outcome)

levels(trainbanking$outcome)<-c(0,1)
levels(testbanking$outcome)<-c(0,1)
#balancing the model using oversampling, undersampling, and both:
bankingbalanced_over<-ovun.sample(outcome~.,data = trainbanking,method = "over",N=nrow(trainbanking))$data
bankingbalanced_under<-ovun.sample(outcome~.,data=trainbanking,method="under", N=((sum(trainbanking[,"outcome"]==1))*2))$data
banking_rose<-ROSE(as.factor(outcome)~.,data=trainbanking,seed=1)$data


#see new balanced datasets:
table(bankingbalanced_over$outcome)
table(bankingbalanced_under$outcome)
table(banking_rose$outcome)

#now I will see how a model performs for each:
tree.bankingbalanced_over<-rpart(outcome~.,data=bankingbalanced_over)
tree.bankingbalanced_under<-rpart(outcome~.,data=bankingbalanced_under)
tree.banking_rose<-rpart(outcome~.,data=banking_rose)

pred.tree.over<-predict(tree.bankingbalanced_over,newdata=testbanking)
pred.tree.under<-predict(tree.bankingbalanced_under,newdata=testbanking)
pred.tree.rose<-predict(tree.banking_rose,newdata=testbanking)

#Evaluating the accuracy of the above using roc:

roc.curve(testbanking$outcome,pred.tree.over[,2])
roc.curve(testbanking$outcome,pred.tree.under[,2])
roc.curve(testbanking$outcome,pred.tree.rose[,2])
```
Given the higher AUC for the ROSE model, I will use that set for the training. 

```{r}
#Balancing test data given the findings above:
banking_test_rose<-ROSE(as.factor(outcome)~.,data=testbanking,seed=1)$data

#now, I start modeling
bankinglogit<-glm(outcome~.,data = banking_rose,family="binomial")
summary(bankinglogit)
Rsq<-1-bankinglogit$deviance/bankinglogit$null.deviance
Rsq
```
R-squared for the logistic model is 0.2172. Now, I will look into a tree model based on the variables more relevant in the logit regression model just to limit the size of the tree.
```{r}
library(tree)
banking_rose2<-banking_rose[,-12]
bankingtree<-tree(factor(outcome)~.,data=banking_rose2)

#bankingtree<-tree(factor(outcome)~.,data=banking)
summary(bankingtree)
plot(bankingtree)
text(bankingtree,label="yval")
plot(bankingtree)
text(bankingtree,label="yprob")



```
The misclassification rate of the model is 0.23. The variables with more information gain for the model is euribor3M, monthContacted, previousCampContact, and CPI. 

Now looking at the confusion matrix for the model. 

```{r}
#install.packages("ROCR")
library(ROCR)
library(caret)

tree.predict<-predict(bankingtree,banking_test_rose)[,2]
tree.predict<-ifelse(tree.predict>0.5,"yes","no")
#confusionMatrix(as.factor(tree.predict),as.factor(banking_test_rose$outcome),positive = "yes")
```

```{r}

#prediction and ROC using the bootstraped balanced test set:
prob<-predict(bankinglogit, newdata=banking_test_rose, type="response")
prob<-ifelse(prob>0.5,1,0)
pred<-prediction(prob, banking_test_rose$outcome)
perf<-performance(pred,measure="tpr",x.measure="fpr")
plot(perf)

confusionMatrix(as.factor(prob),banking_test_rose$outcome)

auc<-performance(pred,measure="auc")
auc<-auc@y.values[[1]]
auc

```
The AUC enables us to see clearly how the different thresholds I tried before affect the accuracy of the model when it comes to the FPR and TPR. AUC is 0.78 which I think it is good for the model. I will continue with that model. 

Now, i will run a random forest to see whether there are improvements to the previous model. I will try to fine tune the forest by leaving nodesize constant and trying different ntree values until I find the one that minimizes the OOB error rate:

```{r}
library(randomForest)

banking.random<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=100, mtry=4)
banking.random2<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=200, mtry=4)
banking.random3<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=300, mtry=4)
banking.random4<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=400, mtry=4)
banking.random5<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=500, mtry=4)

banking.random
banking.random2
banking.random3
banking.random4
banking.random5

```

Lowest error rate achieved with 400 ntrees (10.15%). Using this I will try 3 different nodesizes to check if one is best than the other in terms of the error rate:
```{r}
random.predict<-predict(banking.random4,banking_test_rose)
confusionMatrix(as.factor(random.predict),as.factor(banking_test_rose$outcome),positive="1")

#most important variables according to the fifth random forest model with 5 nodesizes:
varImpPlot(banking.random4)

banking.random4_2<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=400, mtry=3)
banking.random4_3<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=400, mtry=2)
banking.random4_4<-randomForest(outcome~., data=banking_rose, nodesize=5, ntree=400, mtry=5)

banking.random4
banking.random4_2
banking.random4_3
banking.random4_4
```
The tree with the least error is the one with 400 trees and 5 mtries. Mtry is the number of variables available for splitting in each node. I will look at its confusion matrix to check other data:
```{r}
random.predict4<-predict(banking.random4_4,banking_test_rose)
confusionMatrix(as.factor(random.predict4),as.factor(banking_test_rose$outcome),positive="1")
```

As seen above, the error rate for the forest with 5 mtries is lower and accuracy is slightly higher. That's the model I will continue to use for my analysis.


Next, I will start performing XGBoost to see if there is improvement to the model accuracy:

```{r}
library(xgboost)
library(MLmetrics)
library(Matrix)
library(caret)


bankinglabel<-banking_rose$outcome
bankingtestlabel<-testbanking$outcome

new_train<-model.matrix(~.+0,data=banking_rose[,-20])
new_test<-model.matrix(~.+0,data=testbanking[,-20])

bankinglabel<-as.numeric(bankinglabel)-1
bankingtestlabel<-as.numeric(bankingtestlabel)-1
bankingtestlabel2<-banking_test_rose$outcome

trainMatrix<-xgb.DMatrix(data=new_train,label=bankinglabel)
testMatrix<-xgb.DMatrix(data=new_test,label=bankingtestlabel)

#setting up the default parameters as a baseline:

parameters<-list(booster="gbtree",objective="binary:logistic",eta=0.3,gamma=0,max_depth=6,min_child_weight=1,subsample=1,colsample_bytree=1)

#calculate best nround for the model:
xgbcv<-xgb.cv(params = parameters,data=trainMatrix, nrounds=100,nfold=5,showsd = T,stratified = T,print_every_n = 20,early_stopping_rounds = 20,maximize = F)

xgbbanking<-xgb.train(params=parameters,data=trainMatrix,nrounds=20, watchlist=list(val=testMatrix,train=trainMatrix),print_every_n = 5,early_stopping_rounds = 10,maximize = F,eval_metric="error")


```
Now to evaluate the mode: 
```{r}
bankingtestlabel2<-banking_test_rose$outcome
new_test2<-model.matrix(~.+0,data=banking_test_rose[,-20])
bankingtestlabel2<-as.numeric(bankingtestlabel2)-1
testMatrix2<-xgb.DMatrix(data=new_test2,label=bankingtestlabel2)

xgbprediction<-predict(xgbbanking,testMatrix2)
xgbprediction<-ifelse(xgbprediction>0.5,1,0)
confusionMatrix(as.factor(xgbprediction),as.factor(bankingtestlabel2),positive="1")
```
To end the analysis, I will try a final XGBoost model to check whether accuracy is higher:

```{r}
trainMatrix2<-sparse.model.matrix(outcome~.,data=banking_rose)
testMatrix3<-sparse.model.matrix(outcome~.,data = banking_test_rose)
bankingtestlabel3<-as.numeric(banking_test_rose$outcome)-1

xgbbanking1<-xgboost(data=trainMatrix2,
                    label=as.integer(banking_rose$outcome)-1,
                    eta=0.1,
                    max_depth=7,
                    nround=100,
                    print_every_n=50,
                    subsample=0.5,
                    colsample_bytree=0.5,
                    eval_metric="merror",
                    objective="multi:softmax",
                    num_class=2,
                    nthread=3)

xgbprediction2<-predict(xgbbanking1,testMatrix3,type="response")
confusionMatrix(as.factor(xgbprediction2),as.factor(bankingtestlabel3),positive="1")

```
This second model has lower accuracy of 85.63% and higher sensitivity and specificity compared to the previous one. I will use this model for my comparisons and final evaluation and deployment.


I will check its AUC and compare it to the other two models. If the AUC is better, I will try to fine tune the model to increase its accuracy. 

```{r}
library(pROC)
auc

tree.predict<-predict(bankingtree,banking_test_rose,type="class")
tree.prediction<-prediction(as.numeric(tree.predict),banking_test_rose$outcome)
tree.performance<-performance(tree.prediction,measure = "auc")
tree.AUC<-tree.performance@y.values[[1]]
tree.AUC

random.predict4<-predict(banking.random4_4,banking_test_rose, type="response")
probrandombanking<-as.numeric(random.predict4)
predictionrandombanking<-prediction(probrandombanking,banking_test_rose$outcome)
performancerandombanking<-performance(predictionrandombanking,measure = "auc")
AUCrandombanking<-performancerandombanking@y.values[[1]]
AUCrandombanking

xgbbankingprob2<-predict(xgbbanking1,testMatrix3,type="prob")
xgbbankingROC2<-roc(predictor=xgbbankingprob2,response=banking_test_rose$outcome,levels = rev(levels(testbanking$outcome)))
xgbbankingROC2$auc


```
